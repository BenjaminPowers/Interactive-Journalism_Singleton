<html>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- below is the link to the css.style file, which links the HTML and CSS-->

<head>
	<link rel="stylesheet" href="style.css" type="text/css">

</head>

<body>

<header>

  <!-- H1 and H2 denominate the headline and sub hed-->

	<h1>The Black Box Problem with AI</h1>
	<h2> We are using AI for more and more things, but we don't actually know why it makes the decisions it does. </h2>

</header>

<!-- Below are the breakdowns of section, aside, and article, which are ways to float images and text next to each other, which is governed by the CSS file -->

  <section><hr></section>
  <section>
    <aside>
      <img src="Wire.png" width="100%">
    

</aside>
    <article>
      <h3>AI and algorithms are created by feeding information into a computer, feeding lots and lots of information into a computer.</h3>
      <p> Whether it be images, text, or other things, training computers to make decisions based on the information provided to them is, essentially, what an algorithm is.</p>
      
       </article>
  </section>

 <section><hr></section>
  <section>
    <aside>
      <img src="Black_Box.png" width="100%">
    

</aside>
    <article>
      <h3>The problem though, is that we don't know how the computer is arriving at the decision it tells you to execute at the end of this process</h3>
      <p> This is because while we have gotten better at layering neural nets on neural net to an ever increasing extent, we've not reverse engineered a way for computer to explain their decicion making process based on the available data to us. So humans are essentially in the dark about why computers make the final decisions they do.</p>
      
       </article>
  
  </section>

  <section><hr></section>
  <section>
    <aside>
      <img src="Computer.png" width="100%">
    

</aside>
    <article>
      <h3>Given this, we are left looking at a computer at the end of the day, that is unable to explain why it reached the decision it did, which has numerous implications.</h3>
      <p> Various Department of Defense departments such as the Defense Advanced Research Projects Agency are working on "Explainable AI" projects, hoping to illuminate the black box problem we're encountering. The head of the program has identified three basic attempts for doing so. One is deep explanation, essentially an inversion of deep learning, wherein the machine uses machine learning to examine and then analyze its explanation. Another strategy is to use different machine learning techniques that are inherently more interpretable. The simplest way acts something like a standard decision tree. The last method is what the program head calls model induction. “You don’t try to understand the internal logic of the machine learning system,” he says. “You treat it like a black box, but your explanation system can experiment with the black box. It will run a million simulation examples of all different input combinations and look at the outputs to see if you can infer a model that explains and predicts what the system is doing.” While we aren't there yet, it will be interesting to see how this project is resolved in the future

      </p>
      
       </article>
  </section>



</body>



</html>